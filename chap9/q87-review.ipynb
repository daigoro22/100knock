{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss_train: 1.1824 , acc_train: 0.5505 , loss_valid: 1.1952 , acc_valid: 0.5337\n",
      "epoch: 2, loss_train: 1.1457 , acc_train: 0.5933 , loss_valid: 1.1717 , acc_valid: 0.5600\n",
      "epoch: 3, loss_train: 1.1383 , acc_train: 0.5940 , loss_valid: 1.1757 , acc_valid: 0.5615\n",
      "epoch: 4, loss_train: 1.1126 , acc_train: 0.6231 , loss_valid: 1.1416 , acc_valid: 0.5937\n",
      "epoch: 5, loss_train: 1.0797 , acc_train: 0.6600 , loss_valid: 1.1171 , acc_valid: 0.6184\n",
      "epoch: 6, loss_train: 1.0687 , acc_train: 0.6727 , loss_valid: 1.1232 , acc_valid: 0.6184\n",
      "epoch: 7, loss_train: 1.0615 , acc_train: 0.6834 , loss_valid: 1.1071 , acc_valid: 0.6349\n",
      "epoch: 8, loss_train: 1.0570 , acc_train: 0.6867 , loss_valid: 1.1132 , acc_valid: 0.6259\n",
      "epoch: 9, loss_train: 1.0520 , acc_train: 0.6922 , loss_valid: 1.1249 , acc_valid: 0.6117\n",
      "epoch: 10, loss_train: 1.0414 , acc_train: 0.7012 , loss_valid: 1.1154 , acc_valid: 0.6184\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from pytorch_model_summary import summary\n",
    "import pickle\n",
    "#from q86 import my_CNN\n",
    "from q86 import read_csv\n",
    "from q86 import my_Dataset\n",
    "from q85 import calculate_loss_and_accuracy\n",
    "from q85 import padding_batch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class my_CNN(torch.nn.Module):\n",
    "    def __init__(self,dw,dh,n_vocab,padding_idx,max_len):\n",
    "        super().__init__()\n",
    "        #self.emb = torch.nn.Embedding(n_vocab,dw,padding_idx,max_len)\n",
    "        self.emb = torch.nn.Embedding(n_vocab,dw)\n",
    "        self.conv = torch.nn.Conv1d(dw,dh,3,padding=1)#時刻t-1,t,t+1のベクトルを連結した行列に対し、重み行列との積を取る。\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.pool = torch.nn.MaxPool1d(max_len)#各時刻の特徴ベクトルの中から、最大のものを取り出す。\n",
    "        self.linear = torch.nn.Linear(dh,4)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "    def forward(self, x, h=None):\n",
    "        x=self.emb(x)\n",
    "        x=x.view(x.shape[0], x.shape[2], x.shape[1])\n",
    "        x=self.conv(x)\n",
    "        x=self.relu(x)\n",
    "        x=x.view(x.shape[0], x.shape[1], x.shape[2])\n",
    "        x=self.pool(x)\n",
    "        x=x.view(x.shape[0], x.shape[1])\n",
    "        y=self.linear(x)\n",
    "        y=self.softmax(y)\n",
    "        return y\n",
    "\n",
    "def train_model(output_size,vocab_size,padding_idx):\n",
    "    train=read_csv(\"train.csv\")\n",
    "    valid=read_csv(\"valid.csv\")\n",
    "    test=read_csv(\"test.csv\")\n",
    "    dataset_train=my_Dataset(train[\"TITLE\"],train[\"CATEGORY\"])\n",
    "    dataset_valid=my_Dataset(valid[\"TITLE\"],valid[\"CATEGORY\"])\n",
    "    dataset_test=my_Dataset(test[\"TITLE\"],test[\"CATEGORY\"])\n",
    "    \n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True,collate_fn=padding_batch)\n",
    "    dataloader_valid = DataLoader(dataset_valid, batch_size=64, shuffle=True,collate_fn=padding_batch)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=64, shuffle=True,collate_fn=padding_batch)\n",
    "        \n",
    "    device=torch.device('cuda')    \n",
    "    \n",
    "    my_model=my_CNN(300,50,vocab_size,padding_idx,10)#モデルの定義\n",
    "    criterion = nn.CrossEntropyLoss()#損失関数の定義\n",
    "    optimizer = torch.optim.SGD(my_model.parameters(), lr=1e-1)#オプティマイザの定義\n",
    "    \n",
    "    num_epochs=10\n",
    "    log_train = []\n",
    "    log_valid = []\n",
    "    my_model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 訓練モードに設定\n",
    "        my_model.train()\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for data in dataloader_train:\n",
    "            # 勾配をゼロで初期化\n",
    "            optimizer.zero_grad()\n",
    "            # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "            \n",
    "            inputs=data[0].to(device)\n",
    "            labels=data[1].to(device)\n",
    "            \n",
    "            outputs = my_model(inputs)\n",
    "            \n",
    "            loss = criterion(outputs, labels)#損失関数の計算\n",
    "            loss.backward()\n",
    "            optimizer.step()#重みの更新\n",
    "            \n",
    "        #損失関数と正解率を計算        \n",
    "        loss_train, acc_train = calculate_loss_and_accuracy(my_model, criterion, dataloader_train,device)\n",
    "        loss_valid, acc_valid = calculate_loss_and_accuracy(my_model, criterion, dataloader_valid,device)\n",
    "        log_train.append([loss_train, acc_train])\n",
    "        log_valid.append([loss_valid, acc_valid])\n",
    "                         \n",
    "        #ログ出力\n",
    "        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f} , acc_train: {acc_train:.4f} , loss_valid: {loss_valid:.4f} , acc_valid: {acc_valid:.4f}')\n",
    "        \n",
    "    #検証データの損失計算\n",
    "    my_model.eval() \n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(dataloader_valid))\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = my_model(inputs)\n",
    "        loss_valid = criterion(outputs, labels)\n",
    "        \n",
    "    torch.save(my_model.to(device).state_dict(), 'q87_model.pth')#パラメータの保存\n",
    "    return {\"train\": log_train, \"valid\": log_valid}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # パラメータの設定\n",
    "    with open(\"q80_wordIDs.dict\", mode='rb') as f:\n",
    "        word_ID=pickle.load(f)\n",
    "    #print(len(word_ID))\n",
    "    VOCAB_SIZE=len(word_ID) + 1  # 辞書のID数 + パディングID\n",
    "    \n",
    "    \n",
    "    #パディングのサイズ=最長の系列長を求める\n",
    "    df = pd.read_table(\"q80_converted_news.csv\",\n",
    "                       sep=\"\\t\",\n",
    "                       encoding=\"UTF-8\")\n",
    "    PADDING_IDX=0\n",
    "    for IDs in df.TITLE.values.tolist():\n",
    "        \n",
    "        #パディングサイズを更新\n",
    "        if(len(IDs.split())>PADDING_IDX):\n",
    "            PADDING_IDX=len(IDs.split())\n",
    "    \n",
    "    OUTPUT_SIZE = 4\n",
    "    \n",
    "    log=train_model(OUTPUT_SIZE,VOCAB_SIZE,PADDING_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
