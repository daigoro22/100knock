{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8908, 300])\n",
      "195\n",
      "epoch: 1, loss_train: 1.2037 , acc_train: 0.5314 , loss_valid: 1.2144 , acc_valid: 0.5075\n",
      "epoch: 2, loss_train: 1.1479 , acc_train: 0.5828 , loss_valid: 1.1482 , acc_valid: 0.5817\n",
      "epoch: 3, loss_train: 1.0458 , acc_train: 0.6943 , loss_valid: 1.0710 , acc_valid: 0.6747\n",
      "epoch: 4, loss_train: 1.0163 , acc_train: 0.7254 , loss_valid: 1.0466 , acc_valid: 0.6972\n",
      "epoch: 5, loss_train: 0.9956 , acc_train: 0.7463 , loss_valid: 1.0317 , acc_valid: 0.7076\n",
      "epoch: 6, loss_train: 0.9833 , acc_train: 0.7593 , loss_valid: 1.0235 , acc_valid: 0.7166\n",
      "epoch: 7, loss_train: 0.9717 , acc_train: 0.7704 , loss_valid: 1.0154 , acc_valid: 0.7256\n",
      "epoch: 8, loss_train: 0.9650 , acc_train: 0.7780 , loss_valid: 1.0142 , acc_valid: 0.7271\n",
      "epoch: 9, loss_train: 0.9609 , acc_train: 0.7818 , loss_valid: 1.0096 , acc_valid: 0.7301\n",
      "epoch: 10, loss_train: 0.9574 , acc_train: 0.7859 , loss_valid: 1.0168 , acc_valid: 0.7234\n",
      "epoch: 11, loss_train: 0.9568 , acc_train: 0.7862 , loss_valid: 1.0059 , acc_valid: 0.7331\n",
      "epoch: 12, loss_train: 0.9460 , acc_train: 0.7957 , loss_valid: 1.0072 , acc_valid: 0.7331\n",
      "epoch: 13, loss_train: 0.9445 , acc_train: 0.7966 , loss_valid: 1.0047 , acc_valid: 0.7339\n",
      "epoch: 14, loss_train: 0.9449 , acc_train: 0.7969 , loss_valid: 1.0042 , acc_valid: 0.7324\n",
      "epoch: 15, loss_train: 0.9380 , acc_train: 0.8009 , loss_valid: 0.9954 , acc_valid: 0.7444\n",
      "epoch: 16, loss_train: 0.9319 , acc_train: 0.8058 , loss_valid: 0.9996 , acc_valid: 0.7376\n",
      "epoch: 17, loss_train: 0.9249 , acc_train: 0.8196 , loss_valid: 0.9952 , acc_valid: 0.7451\n",
      "epoch: 18, loss_train: 0.9317 , acc_train: 0.8095 , loss_valid: 0.9996 , acc_valid: 0.7324\n",
      "epoch: 19, loss_train: 0.9558 , acc_train: 0.7838 , loss_valid: 1.0287 , acc_valid: 0.7061\n",
      "epoch: 20, loss_train: 0.9326 , acc_train: 0.8100 , loss_valid: 0.9912 , acc_valid: 0.7444\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from pytorch_model_summary import summary\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "from q81 import ID2List\n",
    "from q81 import read_csv\n",
    "from gensim.models import KeyedVectors\n",
    "import numpy as np\n",
    "\n",
    "class my_RNN(nn.Module):\n",
    "    def __init__(self,hidden_size, output_size,vocab_size,batch_size,device,emb_weight):\n",
    "        super().__init__()\n",
    "        self.device=device\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size =batch_size\n",
    "        self.emb = nn.Embedding(vocab_size,300)#単語IDから、ランダムな300個の特徴量を生成\n",
    "        #self.emb = nn.Embedding.from_pretrained(emb_weight)\n",
    "        self.rnn = nn.RNN(300,hidden_size,nonlinearity='relu',bidirectional=True,batch_first=True,num_layers=3)#五つ前までの状態を保持\n",
    "        self.fc = nn.Linear(hidden_size*2, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        self.batch_size=x.size()[0]\n",
    "        hidden=self.initHidden().to(self.device)#隠れ層を初期化\n",
    "        emb = self.emb(x)\n",
    "        #emb.size() = (batch_size, seq_len, emb_size)\n",
    "        out,hidden= self.rnn(emb, hidden)\n",
    "        #out.size() = (batch_size, seq_len, hidden_size)\n",
    "        #print(len(out[1]))\n",
    "        out=self.fc(out[:, int(len(out[1])/2), :])#中央にある単語をfc層に入力する。\n",
    "#         out=self.fc(out[:, -1, :])#最後の単語の部分(xT)だけを取り出して、fc層に入力する。\n",
    "        out=self.softmax(out)\n",
    "        #out.size() = (batch_size, output_size)\n",
    "        return out\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(3*2,self.batch_size,self.hidden_size)\n",
    "\n",
    "\n",
    "\n",
    "def calculate_loss_and_accuracy(model, criterion, loader,device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    loss = 0.0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for i,data in enumerate(loader):\n",
    "            \n",
    "            inputs=data[0].to(device)\n",
    "            labels=data[1].to(device)\n",
    "            outputs = model(inputs)\n",
    "            outputs.to(device)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            pred = torch.argmax(outputs, dim=-1)\n",
    "            total += len(inputs)\n",
    "            correct += (pred == labels).sum().item()\n",
    "            \n",
    "    return loss / len(loader), correct / total     \n",
    "    \n",
    "\n",
    "class my_Dataset(Dataset):\n",
    "    def __init__(self,X,Y):#初期化時に実行される関数\n",
    "        self.X=X\n",
    "        self.Y=Y\n",
    "        \n",
    "    def __len__(self):#len()関数で、返す値を決定する関数\n",
    "        return len(self.Y)\n",
    "    \n",
    "    def __getitem__(self,idx):#インデックス指定で、返すものを決定する関数\n",
    "        return torch.tensor(self.X[idx]),torch.tensor(self.Y[idx])\n",
    "    \n",
    "    \n",
    "def padding_batch(batch):\n",
    "    padding_size=0\n",
    "    padding_idx=8909\n",
    "    \n",
    "    sequences = [x[0] for x in batch]#データ部を取り出す\n",
    "    labels = torch.LongTensor([x[1] for x in batch])#ラベル部を取り出す\n",
    "    sequences_padded=torch.nn.utils.rnn.pad_sequence(sequences, batch_first=True, padding_value=padding_idx)#後ろをパディングIDでパディング\n",
    "    \n",
    "    return sequences_padded,labels\n",
    "\n",
    "\n",
    "\n",
    "def train_model(hidden_size,output_size,vocab_size,BATCH_SIZE,EMB_WEIGHT):\n",
    "    train=read_csv(\"train.csv\")\n",
    "    valid=read_csv(\"valid.csv\")\n",
    "    test=read_csv(\"test.csv\")\n",
    "    dataset_train=my_Dataset(train[\"TITLE\"],train[\"CATEGORY\"])\n",
    "    dataset_valid=my_Dataset(valid[\"TITLE\"],valid[\"CATEGORY\"])\n",
    "    dataset_test=my_Dataset(test[\"TITLE\"],test[\"CATEGORY\"])\n",
    "    \n",
    "    dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=False,collate_fn=padding_batch)\n",
    "    dataloader_valid = DataLoader(dataset_valid, batch_size=BATCH_SIZE, shuffle=False,collate_fn=padding_batch)\n",
    "    dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=False,collate_fn=padding_batch)\n",
    "        \n",
    "    device=torch.device('cuda')    \n",
    "    \n",
    "    my_model=my_RNN(hidden_size,output_size,vocab_size,BATCH_SIZE,device,EMB_WEIGHT)#モデルの定義\n",
    "    criterion = nn.CrossEntropyLoss()#損失関数の定義\n",
    "    optimizer = torch.optim.SGD(my_model.parameters(), lr=1e-1)#オプティマイザの定義\n",
    "    \n",
    "    num_epochs=20\n",
    "    log_train = []\n",
    "    log_valid = []\n",
    "    my_model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        # 訓練モードに設定\n",
    "        my_model.train()\n",
    "        loss_train = 0.0\n",
    "        \n",
    "        for data in dataloader_train:\n",
    "            # 勾配をゼロで初期化\n",
    "            optimizer.zero_grad()\n",
    "            # 順伝播 + 誤差逆伝播 + 重み更新\n",
    "            \n",
    "            inputs=data[0].to(device)\n",
    "            labels=data[1].to(device)\n",
    "            \n",
    "            outputs = my_model(inputs)\n",
    "#            print(outputs)\n",
    "#             print(f\"{type(outputs)}:{outputs}\")\n",
    "#             print(f\"{type(labels)}:{labels}\")\n",
    "            loss = criterion(outputs, labels)#損失関数の計算\n",
    "            loss.backward()\n",
    "            optimizer.step()#重みの更新\n",
    "            \n",
    "        #損失関数と正解率を計算        \n",
    "        loss_train, acc_train = calculate_loss_and_accuracy(my_model, criterion, dataloader_train,device)\n",
    "        loss_valid, acc_valid = calculate_loss_and_accuracy(my_model, criterion, dataloader_valid,device)\n",
    "        log_train.append([loss_train, acc_train])\n",
    "        log_valid.append([loss_valid, acc_valid])\n",
    "                         \n",
    "        #ログ出力\n",
    "        print(f'epoch: {epoch + 1}, loss_train: {loss_train:.4f} , acc_train: {acc_train:.4f} , loss_valid: {loss_valid:.4f} , acc_valid: {acc_valid:.4f}')\n",
    "        \n",
    "    #検証データの損失計算\n",
    "    my_model.eval() \n",
    "    with torch.no_grad():\n",
    "        inputs, labels = next(iter(dataloader_valid))\n",
    "        inputs=inputs.to(device)\n",
    "        labels=labels.to(device)\n",
    "        outputs = my_model(inputs)\n",
    "        loss_valid = criterion(outputs, labels)\n",
    "        \n",
    "    torch.save(my_model.to(device).state_dict(), 'q84_model.pth')#パラメータの保存\n",
    "    return {\"train\": log_train, \"valid\": log_valid}\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    with open(\"q80_wordIDs.dict\", mode='rb') as f:\n",
    "        word_ID=pickle.load(f)\n",
    "        \n",
    "        \n",
    "    model=KeyedVectors.load('q84_model.pt', mmap='r')\n",
    "    \n",
    "    #VOCAB_SIZE=len(model) + 1  # 辞書のID数 + パディングID\n",
    "    VOCAB_SIZE=len(word_ID) + 1  # 辞書のID数 + パディングID\n",
    "    emb_weight=np.zeros((VOCAB_SIZE,300))\n",
    "    \n",
    "    for i,key in enumerate(word_ID.keys()):\n",
    "        \n",
    "        if key in model:\n",
    "            emb_weight[i]=model[key]#モデル内に存在すれば、その特徴量で初期化\n",
    "        else:\n",
    "            #emb_weight[i]=np.random.normal(scale=0.4, size=(300,))#モデル内に存在しなければ、適当な値(正規分布)で初期化\n",
    "            emb_weight[i]=np.random.normal(loc=0.0,scale=0.4, size=(300,))#モデル内に存在しなければ、適当な値(正規分布)で初期化\n",
    "            #emb_weight[i]=np.zeros((300))#モデル内に存在しなければ、適当な値(正規分布)で初期化\n",
    "    emb_weight = torch.from_numpy(emb_weight.astype((np.float32)))#torch.tensor型に変換(隠れ層と合わせるため、float型に変換)\n",
    "    \n",
    "    df = pd.read_table(\"q80_converted_news.csv\",sep=\"\\t\",encoding=\"UTF-8\")\n",
    "    PADDING_IDX=0    \n",
    "    for IDs in df.TITLE.values.tolist():\n",
    "        #パディングサイズを更新\n",
    "        if(len(IDs.split())>PADDING_IDX):\n",
    "            PADDING_IDX=len(IDs.split())\n",
    "    \n",
    "    \n",
    "    print(emb_weight.shape)\n",
    "    print(PADDING_IDX)\n",
    "    log=train_model(50,4,VOCAB_SIZE,64,emb_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
