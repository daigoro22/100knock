{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#参考サイト\n",
    "#https://qiita.com/ground0state/items/155b77f4c07e1a509a14\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "#カンマ区切りはread_csv()、タブ区切りはread_tabel()を使用\n",
    "\n",
    "def load_data(filename):\n",
    "    df=pd.read_table(filename,\n",
    "                                   header=None,\n",
    "                                   sep=\"\\t\",\n",
    "                                   encoding=\"UTF-8\")\n",
    "    X=df.drop(df.columns[[len(df.columns)-1]], axis=1)#特徴量の取得\n",
    "    Y=df[len(df.columns)-1]#ラベルの取得\n",
    "    return X,Y\n",
    "\n",
    "df = pd.read_table(\"news_debug.txt\",\n",
    "                                   header=None,\n",
    "                                   names=(\"ID\",\"TITLE\",\"URL\",\"PUBLISHER\",\"CATEGORY\",\"STORY\",\"HOSTNAME\",\"TIME\"),\n",
    "                                   sep=\"\\t\",\n",
    "                                   encoding=\"UTF-8\")\n",
    "instance=df.query(\"PUBLISHER in ['Reuters','Huffington Post','Businessweek','Contactmusic.com','Daily Mail']\")\n",
    "instance_loc=instance.loc[:,[\"CATEGORY\",\"TITLE\"]]\n",
    "category_list=instance_loc.CATEGORY.values.tolist()\n",
    "title_list=instance_loc.TITLE.values.tolist()\n",
    "\n",
    "\n",
    "for i,title in enumerate(title_list):\n",
    "    title=title.lower()\n",
    "    title=re.sub('[!\"#$%&\\'\\\\\\\\()*+,-./:;<=>?@[\\\\]^_`{|}~「」〔〕“”〈〉『』【】＆＊・（）＄＃＠。、？！｀＋￥％]',\"\",title)#記号類を削除\n",
    "    title=re.sub(\" [0-9]+\",\" 0\",title)#数字を0に置き換え\n",
    "    title=re.sub(\" [0-9]+(.+?) \",\" \\\\1 \",title)#3-appleのような表現をappleに置き換え\n",
    "    title=re.sub(\"million|thouzand|billion|trillion|quadrillion\",\"0\",title)#位を示す数詞を0に置き換え\n",
    "    title_list[i]=title\n",
    "        #word=re.sub(\"[0-9]+\",\"0\",word)\n",
    "#print(title_list)\n",
    "#print(len(title_list))\n",
    "\n",
    "#カテゴリ名を数値に変換\n",
    "str_to_int={\"b\":0,\"t\":1,\"e\":2,\"m\":3}\n",
    "for i,category in enumerate(category_list):\n",
    "    category_list[i]=str_to_int[category]  \n",
    "#print(category_list)\n",
    "\n",
    "#データ量を軽くするため、出現頻度の低い単語を無視\n",
    "vec_tfidf=TfidfVectorizer(min_df=0.01)\n",
    "X=vec_tfidf.fit_transform(title_list)\n",
    "\n",
    "\n",
    "\n",
    "print(type(X))\n",
    "df=pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())#pandas.df型に変換\n",
    "df[\"CATEGORY\"]=category_list#カテゴリ名を末尾に追加\n",
    "train,valid=train_test_split(df,test_size=0.20)#訓練データと検証データに分割\n",
    "valid,test=train_test_split(valid,test_size=0.25)#検証データを、検証データと評価データに再分割\n",
    "\n",
    "\n",
    "#ファイル出力\n",
    "train.to_csv(\"train.feature_debug.txt\", sep=\"\\t\",header=False,index=False)\n",
    "valid.to_csv(\"valid.feature_debug.txt\", sep=\"\\t\",header=False,index=False)\n",
    "test.to_csv(\"test,feature_debug.txt\", sep=\"\\t\",header=False,index=False)\n",
    "\n",
    "train_df=pd.read_table(\"train.feature_debug.txt\",\n",
    "                                   header=None,\n",
    "                                   sep=\"\\t\",\n",
    "                                   encoding=\"UTF-8\")\n",
    "\n",
    "X_train=train_df.drop(train_df.columns[[len(df.columns)-1]], axis=1)\n",
    "Y_train=train_df[len(train_df.columns)-1]\n",
    "print(X_train)\n",
    "print(Y_train)\n",
    "lr = LogisticRegression(max_iter=1000)#インスタンスを作成、デフォルトで収束しなかったため1000とした\n",
    "lr.fit(X_train, Y_train)#重みを学習\n",
    "\n",
    "#モデルをシリアライズして保存\n",
    "filename=\"my_lr_debug.model\"\n",
    "pickle.dump(lr, open(filename, 'wb'))\n",
    "\n",
    "lr=pickle.load(open(\"my_lr_debug.model\", 'rb'))\n",
    "\n",
    "X_train,Y_train=load_data(\"train.feature_debug.txt\")\n",
    "X_test,Y_test=load_data(\"test.feature_debug.txt\")\n",
    "pred_train=lr.predict(X_train)\n",
    "pred_test=lr.predict(X_test)\n",
    "#print(pred_train)\n",
    "#print(Y_train)\n",
    "#print(pred_test)\n",
    "#print(Y_test)\n",
    "print(accuracy_score(Y_train, pred_train))#学習データ上での正解率を表示\n",
    "print(accuracy_score(Y_test, pred_test))#評価データ上での正解率を表示\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
